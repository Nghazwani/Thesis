
\section{Introduction}
Electrons and photons deposit their energies in ECAL by showering, via bremsstrahlung or photon conversion, in the crystals. Therefore, the particle's energy spreads through multiple calorimeter clusters extending the phi direction. The reconstructed PF cluster energy from the PF algorithm is expected to be smaller than the true energy of the incoming particle. This loss of energy could be due to tracker material, gaps, dead channels, etc. Therefore, a calibration of the calorimeter cluster energy is needed.  
 
This chapter presents the ML method and datasets calibrating the online and offline PF ECAL cluster.   

\section{XGBoost} %source1  source2 

The ML algorithm used to calibrate the ECAL pf cluster is called XGBoost algorithm. XGBoost stands for Extreme gradient boosting which is a type of gradient boosting. Gradient boosting uses the gradient decent (clarify better – how is calculated) to create new the learners where the loss function, which define the distance between the truth and prediction, is differentiable.  (find the related figure)

this algorithm works by combining the boosting technique and many decision trees to get the final prediction to achieve higher accuracy. During training process: The algorithm starts with an initial prediction and compute the residuals, loss. Then it creates other decision trees using a similarity score for the residuals (clarify better) which are then used to generate the output values for each leaf.  This process is repeated either until the residuals (error) stop reducing or for a specific number of times. In general, each subsequent tree learns from the previous ones.

XGBoost uses assign higher importance to the misclassified sample during the reconstruction of the next tree. Meaning the next tree can focus on correcting those mistakes and by doing that the algorithm improves its accuracy. To prevent overfitting, the algorithm is uses of a regularization term.


\section{Datasets description}
The first part of this thesis focuses on calibrating PF ECAL clusters for Run3. This includes calibrating both online (High Level Trigger) and offline PF ECAL clusters. The samples used here are double-photon with zero-material Monte Carlo (MC) samples. Double-photon increases the statistics, and zero-material means there is no material in front of the calorimeter to eliminate dealing with bremsstrahlung and photon conversions.    

The datasets are centrally produced and reconstructed in CMSSW 13.3.0 (133X) under 2024 conditions. For the offline case, the (miniAOD) samples can be found through the Data Aggregation System (DAS) web page. However, for the special case of checking the online PF calibration, we need to produce a new miniAOD that contains HLT PF ECAL clusters. This is done by re-running Digi->Reco->miniAOD to keep HLT PF ECAL Cluster data.

In general, there are two types of datasets used here, one with Pile up (PU) 80, meaning 80 collisions occurring simultaneously within one p-p bunch crossing source, and another with no pile (NoPU). Both correspond to a center of mass energy 13.6 \TeV with pt range up for offline case to 1500 GeV/c. And 500 \GeV/\c with only about 1000 events for online case.     

Before splitting the data samples into two groups, 80\% training and 20\% testing, the training samples need to be divided into smaller groups. This is done to get better models when training the ML algorithm. The NoPU training samples will be divided first into two categories: Full readout and zero suppression (ZS) (explain difference source). Then, each category will be split further depending on ECAL region and PT range. In the end, we end up with 6 groups for Full readout and 2 groups for ZS readout. (Add table summarizing the sample description.) 




\section{PF ECAL cluster regression}
The ML used for PF ECAL cluster regression is called XGBoost algorithm. (which has been covered in the previous section). The PF cluster regression here is done through two steps: first phase: training the ML algorithm using training data (NoPU).Second phase, testing the ML model using testing data (NoPU, PU).

First, we starting with the training details. To estimate the calibration of the PF ECAL Cluster we consider the case where a one photon has deposited all its energy in one PF cluster in the ECAL, since we have zero material as mentioned before (Approximately 94\% of the incident energy of a single electron or photon is contained in 3x3 crystals, and 97\% in 5x5 crystals source).  
This allows us to calculate the correction factor = Energy of “generated photon” / E “raw PF cluster”.

For the training variables we are using the same variables for Run 2 and Run 3 (2023) calibration. The training input variables are summarized in the related table, this includes:  
ClusrawE (raw energy of the cluster, uncorrected), 
ieta,iphi (polar coordinates for clusters in EB), 
ix, iy (cartesian coordinates for clusters in EE), 
ieta mod20, iphi mod20 (these are polar coordinate of the crystal in which the main cluster detected modulo 20 for clusters in EB), 
(clusPS1+clusPS2)/clusrawE (clusPSi: cluster in pre shower layer i, only EE), 
 number of hits in the cluster (which takes values 1, 2 and 3 - it takes 3 if n hits >= 3), 
and the target used in the training is log (Egen/Eraw)  
%(Mostly lognormal distribution in energies? source).

After training is done, we get 8 regression models form each group in the training data. To validated the results of ML training we apply the following steps: 
first, we apply the trained model on the test data to get the value of the correction factor, 
then calculate:  the corrected PF cluster energy = E “raw pf cluster” * correction factor, 
after that we plot: the response: E “corrected PF cluster” / E “gen photon”, 
next we fit the plot with: double CB function for each E “gen photon” bin (notes (for pt< 6 GeV we use Gaus function for fitting since it will be difficult to do the with double CB- in other words low pt (zero suppressed)) 
finally, From the fitted curve we find: mean, effective sigma.  

We can also plot the response: E “raw PF cluster” / E “gen photon”, to compare mean and eff sigma to the corrected PF cluster. (insert an example plots of the fitting)   
\section{results}
(summary)

\subsection{offline PF ECAL cluster calibration}
We checked the 2024 double photon calibration samples for PF ECAL clusters. Generally, the new correction (133X) is very close to the current used calibration (126X).
This shows that the existing calibration derived from 2022 samples seems to continue to working well.
We checked the 2024 double photon calibration samples for PF ECAL clusters. Generally, the new correction (133X) is ver\
y close to the current used calibration (126X). This shows that the existing calibration derived from 2022 samples seem\
s to continue to working well. (Add comments about HLT vs offline calibration)

\subsubsection{ECAL Barrel}
first, we are presenting the results of testing the ML models on the NOPU sample then the PU case.
These plots show response (resolution) vs Pt gen in GeV and in their corresponding eta range.
\include{./plots_tex/PU_EB_FULL_plots}
\include{./plots_tex/PU_EB_ZS_plots}
\include{./plots_tex/NOPU_EB_FULL_plots}
\include{./plots_tex/NOPU_EB_ZS_plots}

\subsubsection{ECAL Endcap}

\include{./plots_tex/NOPU_EE_FULL_plots}
\include{./plots_tex/NOPU_EE_ZS_plots}
\include{./plots_tex/PU_EE_FULL_plots}
\include{./plots_tex/PU_EE_ZS_plots}

\subsection{HLT vs offline PF ECAL cluster}

(connect to PF chapter where where we mentioned offline PF)
(also mention the procution of a new ntuple that containes HLT information) 

first for NoPU samples
\include{./plots_tex/NoPU_HLT_offline_plots}
Second for PUU samples
\include{./plots_tex/PU_HLT_offline_plots}
